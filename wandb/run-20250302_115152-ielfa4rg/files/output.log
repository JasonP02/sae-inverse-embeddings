Loading models with config: hdbscan
Collecting data with n_prompts=100
Loading prompts from WikiText...
Added 100 prompts from WikiText
Total unique prompts loaded: 100
Processing batch 1/11...
Processing batch 2/11...
Processing batch 3/11...
Processing batch 4/11...
Processing batch 5/11...
Processing batch 6/11...
Processing batch 7/11...
Processing batch 8/11...
Processing batch 9/11...
Processing batch 10/11...
Collected activations for 100 prompts, 32768 features
Entropy range: -0.00 to 4.36
Sparsity range: 0.00 to 0.82
Entropy percentiles: 10%=-0.0000, 50%=-0.0000, 90%=3.2167
Sparsity percentiles: 10%=0.0000, 50%=0.0000, 90%=0.0000
Kept 1 out of 32768 features
Saving processed data to cache/processed_data_prompts_100_entropy_0.41_3.49_sparsity_0.20_0.94.pt...
Data saved successfully.
Running clustering with method=hdbscan, UMAP=False
Warning: Only 1 features after filtering. Skipping clustering.
/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:

'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.

/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:

n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
Calculating metrics
Run complete: found 0 clusters, silhouette=-1.0000
