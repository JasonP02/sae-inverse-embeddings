Loading models with config: hdbscan
Collecting data with n_prompts=100
Loading prompts from WikiText...
Added 100 prompts from WikiText
Total unique prompts loaded: 100
Processing batch 1/11...
Processing batch 2/11...
Processing batch 3/11...
Processing batch 4/11...
Processing batch 5/11...
Processing batch 6/11...
Processing batch 7/11...
Processing batch 8/11...
Processing batch 9/11...
Processing batch 10/11...
Collected activations for 100 prompts, 32768 features
Entropy range: -0.00 to 4.36
Sparsity range: 0.00 to 0.76
Entropy percentiles: 10%=-0.0000, 50%=-0.0000, 90%=3.2167
Sparsity percentiles: 10%=0.0000, 50%=0.0000, 90%=0.0000
Kept 17 out of 32768 features
Saving processed data to cache/processed_data_prompts_100_entropy_0.20_4.59_sparsity_0.07_0.91.pt...
Data saved successfully.
Running clustering with method=hdbscan, UMAP=True
Applying UMAP reduction to 17 features â†’ 25 dimensions
/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:

'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.

/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:

n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.

/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/umap/umap_.py:2462: UserWarning:

n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1

/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/umap/spectral.py:519: RuntimeWarning:

k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.
Error during UMAP reduction: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
Falling back to normalized features without dimensionality reduction
Error in sweep run: name 'HDBSCAN_AVAILABLE' is not defined
Traceback (most recent call last):
  File "/home/j/Projects/sae-inverse-embeddings/clustering_sweep.py", line 284, in run_clustering_sweep
    labels, normalized_acts = run_clustering(
  File "/home/j/Projects/sae-inverse-embeddings/pipeline.py", line 116, in run_clustering
    labels, reduced_acts = cluster_features(filtered_acts, clustering_config)
  File "/home/j/Projects/sae-inverse-embeddings/clustering.py", line 127, in cluster_features
    # Use HDBSCAN if available and selected
NameError: name 'HDBSCAN_AVAILABLE' is not defined
