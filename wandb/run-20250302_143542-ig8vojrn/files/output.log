Loading models with config: hdbscan
Collecting data with n_prompts=1000
Loading prompts from WikiText...
Added 1000 prompts from WikiText
Total unique prompts loaded: 985
Processing batch 1/99...
Processing batch 2/99...
Processing batch 3/99...
Processing batch 4/99...
Processing batch 5/99...
Processing batch 6/99...
Processing batch 7/99...
Processing batch 8/99...
Processing batch 9/99...
Processing batch 10/99...
Processing batch 11/99...
Processing batch 12/99...
Processing batch 13/99...
Processing batch 14/99...
Processing batch 15/99...
Processing batch 16/99...
Processing batch 17/99...
Processing batch 18/99...
Processing batch 19/99...
Processing batch 20/99...
Processing batch 21/99...
Processing batch 22/99...
Processing batch 23/99...
Processing batch 24/99...
Processing batch 25/99...
Processing batch 26/99...
Processing batch 27/99...
Processing batch 28/99...
Processing batch 29/99...
Processing batch 30/99...
Processing batch 31/99...
Processing batch 32/99...
Processing batch 33/99...
Processing batch 34/99...
Processing batch 35/99...
Processing batch 36/99...
Processing batch 37/99...
Processing batch 38/99...
Processing batch 39/99...
Processing batch 40/99...
Processing batch 41/99...
Processing batch 42/99...
Processing batch 43/99...
Processing batch 44/99...
Processing batch 45/99...
Processing batch 46/99...
Processing batch 47/99...
Processing batch 48/99...
Processing batch 49/99...
Processing batch 50/99...
Processing batch 51/99...
Processing batch 52/99...
Processing batch 53/99...
Processing batch 54/99...
Processing batch 55/99...
Processing batch 56/99...
Processing batch 57/99...
Processing batch 58/99...
Processing batch 59/99...
Processing batch 60/99...
Processing batch 61/99...
Processing batch 62/99...
Processing batch 63/99...
Processing batch 64/99...
Processing batch 65/99...
Processing batch 66/99...
Processing batch 67/99...
Processing batch 68/99...
Processing batch 69/99...
Processing batch 70/99...
Processing batch 71/99...
Processing batch 72/99...
Processing batch 73/99...
Processing batch 74/99...
Processing batch 75/99...
Processing batch 76/99...
Processing batch 77/99...
Processing batch 78/99...
Processing batch 79/99...
Processing batch 80/99...
Processing batch 81/99...
Processing batch 82/99...
Processing batch 83/99...
Processing batch 84/99...
Processing batch 85/99...
Processing batch 86/99...
Processing batch 87/99...
Processing batch 88/99...
Processing batch 89/99...
Processing batch 90/99...
Processing batch 91/99...
Processing batch 92/99...
Processing batch 93/99...
Processing batch 94/99...
Processing batch 95/99...
Processing batch 96/99...
Processing batch 97/99...
Processing batch 98/99...
Processing batch 99/99...
Collected activations for 985 prompts, 32768 features
Entropy range: -0.00 to 6.68
Sparsity range: 0.00 to 0.65
Entropy percentiles: 10%=-0.0000, 50%=-0.0000, 90%=5.5664
Sparsity percentiles: 10%=0.0000, 50%=0.0000, 90%=0.0000
Kept 11 out of 32768 features
Saving processed data to cache/processed_data_prompts_1000_entropy_0.42_6.35_sparsity_0.07_0.97.pt...
Data saved successfully.
Running clustering with method=hdbscan, UMAP=True
Applying UMAP reduction to 11 features â†’ 100 dimensions
/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:

'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.

/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning:

n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.

/home/j/miniconda3/envs/slens/lib/python3.10/site-packages/umap/spectral.py:519: RuntimeWarning:

k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.
Error during UMAP reduction: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
Falling back to normalized features without dimensionality reduction
Error in sweep run: name 'HDBSCAN_AVAILABLE' is not defined
Traceback (most recent call last):
  File "/home/j/Projects/sae-inverse-embeddings/clustering_sweep.py", line 284, in run_clustering_sweep
    labels, normalized_acts = run_clustering(
  File "/home/j/Projects/sae-inverse-embeddings/pipeline.py", line 116, in run_clustering
    labels, reduced_acts = cluster_features(filtered_acts, clustering_config)
  File "/home/j/Projects/sae-inverse-embeddings/clustering.py", line 129, in cluster_features
    if HDBSCAN_AVAILABLE:
NameError: name 'HDBSCAN_AVAILABLE' is not defined
