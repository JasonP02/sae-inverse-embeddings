{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview & Current Questions\n",
    "This is for validating my clustering idea:\n",
    "1. Store SAE activations for all prompts\n",
    "2. Preprocess activations based on entropy and activation level (optional?)\n",
    "3. Cluster activations using UMAP and HDBSCAN\n",
    "4. Analyze clusters\n",
    "\n",
    "Some questions: \n",
    "1. How does entropy, activation preprocessing affect clustering? How does n_prompts, length, affect this step?\n",
    "2. How useful is UMAP? What are the best parameters?\n",
    "3. How does HDBSCAN perform? What are the best parameters?\n",
    "4. How does the clustering change when we use different datasets?\n",
    "5. What commonalities do clusters have? Does this vary by dataset? Hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, config, model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import torch\n",
    "import gc\n",
    "from config import config # cfg auto updates\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap.umap_ as umap\n",
    "import plotly.graph_objects as go\n",
    "from einops import rearrange\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HookedSAETransformer.from_pretrained(\"EleutherAI/pythia-70m-deduped\", device=device)\n",
    "sae, _, _ = SAE.from_pretrained(\n",
    "    release=\"pythia-70m-deduped-mlp-sm\",\n",
    "    sae_id=\"blocks.3.hook_mlp_out\",\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Activations from get_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(filename):\n",
    "    \"\"\"Load processed data from disk.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Loading processed data from {filename}...\")\n",
    "        try:\n",
    "            data = torch.load(filename)\n",
    "            print(\"Data loaded successfully.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No cached data found at {filename}\")\n",
    "        return None\n",
    "    \n",
    "def get_cache_filename(config, n_prompts):\n",
    "    \"\"\"Generate a cache filename based on hierarchical config parameters.\"\"\"\n",
    "    # Create a unique filename based on key parameters\n",
    "    \n",
    "    params = [\n",
    "        f\"prompts_{n_prompts}\",\n",
    "    ]\n",
    "    print(params)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    cache_dir = config.get('cache_dir', 'cache')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    return os.path.join(cache_dir, f\"processed_data_{'_'.join(params)}.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompts_25000']\n",
      "No cached data found at feature_cache/processed_data_prompts_25000.pt\n",
      "True\n",
      "Processing data from scratch with 25000 prompts\n",
      "Loading prompts from wikipedia...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data from scratch with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_prompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m \u001b[43mload_diverse_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     acts \u001b[38;5;241m=\u001b[39m collect_activations(model, sae, prompts, config)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Save the new data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 113\u001b[0m, in \u001b[0;36mload_diverse_prompts\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     text_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m    114\u001b[0m     text \u001b[38;5;241m=\u001b[39m item[text_key]\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# Handle list-type fields (e.g., dialogues)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/site-packages/datasets/iterable_dataset.py:2033\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[0;32m-> 2033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m:\n\u001b[1;32m   2034\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   2037\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   2038\u001b[0m         )\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m format_dict(example) \u001b[38;5;28;01mif\u001b[39;00m format_dict \u001b[38;5;28;01melse\u001b[39;00m example\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/site-packages/datasets/arrow_dataset.py:208\u001b[0m, in \u001b[0;36mDatasetInfoMixin.features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeatures\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Features]:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/site-packages/datasets/features/features.py:2110\u001b[0m, in \u001b[0;36mFeatures.copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;124;03m    Make a deep copy of [`Features`].\u001b[39;00m\n\u001b[1;32m   2095\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;66;03m# aha, this is the first one :-)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m         memo[\u001b[38;5;28mid\u001b[39m(memo)]\u001b[38;5;241m=\u001b[39m[x]\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reconstruct\u001b[39m(x, memo, func, args,\n\u001b[1;32m    260\u001b[0m                  state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, listiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dictiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m                  \u001b[38;5;241m*\u001b[39m, deepcopy\u001b[38;5;241m=\u001b[39mdeepcopy):\n\u001b[1;32m    262\u001b[0m     deep \u001b[38;5;241m=\u001b[39m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Determine whether to use cached data \n",
    "n_prompts = config['n_prompts']\n",
    "use_cached_data = config['use_cached_data']\n",
    "cache_filename = get_cache_filename(config, n_prompts)\n",
    "cached_data = load_processed_data(cache_filename)\n",
    "\n",
    "\n",
    "acts = cached_data['acts']\n",
    "prompts = cached_data.get('prompts', [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering using entropy and sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_sparsity_varentropy(acts, config, n_prompts):\n",
    "    \n",
    "    # Calculate entropy and sparsity\n",
    "    activations = acts.abs()\n",
    "    probs = activations / (activations.sum(dim=0) + 1e-10)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=0)\n",
    "\n",
    "    # Correct varentropy calculation\n",
    "    # We want variance of entropy across prompts for each feature\n",
    "    log_probs = torch.log(probs + 1e-10)\n",
    "    entropy_per_prompt = -(probs * log_probs)  # Shape: [n_prompts, n_features]\n",
    "    mean_entropy = entropy_per_prompt.mean(dim=0)  # Shape: [n_features]\n",
    "    varentropy = torch.mean((entropy_per_prompt - mean_entropy.unsqueeze(0))**2, dim=0)  # Shape: [n_features]\n",
    "    # variance of entropy is a measure of how much the entropy varies across prompts \n",
    "    # i posit that this will be useful for clustering\n",
    "    \n",
    "    activation_threshold = config.get('activation_threshold', 0.1)\n",
    "    sparsity = (acts.abs() > activation_threshold).float().mean(dim=0)\n",
    "\n",
    "    # if config['verbose']:\n",
    "    #     # After calculating entropy and sparsity\n",
    "    #     print(f\"\\nEntropy stats:\")\n",
    "    #     print(f\"Min: {entropy.min().item():.3f}\")\n",
    "    #     print(f\"Max: {entropy.max().item():.3f}\")\n",
    "    #     print(f\"Mean: {entropy.mean().item():.3f}\")\n",
    "\n",
    "    #     print(f\"\\nSparsity stats:\")\n",
    "    #     print(f\"Min: {sparsity.min().item():.3f}\")\n",
    "    #     print(f\"Max: {sparsity.max().item():.3f}\")\n",
    "    #     print(f\"Mean: {sparsity.mean().item():.3f}\")\n",
    "\n",
    "    #     # Print the thresholds being used\n",
    "    #     print(f\"\\nThresholds:\")\n",
    "    #     print(f\"Entropy: [{config['entropy_threshold_low']}, {config['entropy_threshold_high']}]\")\n",
    "    #     print(f\"Sparsity: [{config['sparsity_min']}, {config['sparsity_max']}]\")\n",
    "\n",
    "    return entropy, sparsity, varentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(acts, config):\n",
    "    \"\"\"\n",
    "    Filter features based on entropy and sparsity.\n",
    "    We are looking at the feature behavior across prompts (we could modify this for tokens, or even for tokens within prompts)\n",
    "    \"\"\"\n",
    "\n",
    "    n_prompts, n_features = acts.shape\n",
    "    entropy, sparsity, varentropy = get_entropy_sparsity_varentropy(acts, config, n_prompts)\n",
    "    \n",
    "    # Apply filtering mask\n",
    "    mask = (entropy > config['entropy_threshold_low']) & \\\n",
    "           (entropy < config['entropy_threshold_high']) & \\\n",
    "           (sparsity > config['sparsity_min']) & \\\n",
    "           (sparsity < config['sparsity_max'])\n",
    "    \n",
    "    print(f\"Kept {mask.sum().item()} out of {n_features} features\")\n",
    "    return acts[:, mask], mask.nonzero(as_tuple=True)[0], entropy, sparsity, varentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_acts, original_indices, entropy_data, sparsity_data, varentropy_data = filter_features(acts, config)\n",
    "print(f' after filtering acts.shape: {acts.shape}')\n",
    "\n",
    "if config['visualize_features']:\n",
    "    # Create a DataFrame for entropy and sparsity\n",
    "    df = pd.DataFrame({\n",
    "        'entropy': entropy_data.cpu().numpy(),\n",
    "        'sparsity': sparsity_data.cpu().numpy(),\n",
    "        'varentropy': varentropy_data.cpu().numpy(),\n",
    "    })\n",
    "\n",
    "    px.scatter(df, x='entropy', y='sparsity').show()\n",
    "    px.scatter(df, x='entropy', y='varentropy').show()\n",
    "    px.scatter(df, x='sparsity', y='varentropy').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and sparsity visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_quadrants(filtered_acts, n_examples=5):\n",
    "    \"\"\"Analyze features based on their entropy/varentropy quadrants.\n",
    "    \n",
    "    Args:\n",
    "        acts: [n_prompts, n_features] activation tensor\n",
    "        entropy: [n_features] entropy per feature\n",
    "        varentropy: [n_features] variance of entropy per feature\n",
    "        n_examples: number of example features to show per quadrant\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_entropy, filtered_sparsity, filtered_varentropy = get_entropy_sparsity_varentropy(filtered_acts, config, n_prompts)\n",
    "\n",
    "    # Get medians for quadrant splitting\n",
    "    entropy_median = filtered_entropy.median()\n",
    "    varentropy_median = filtered_varentropy.median()\n",
    "    \n",
    "    # Create quadrant masks\n",
    "    q1 = (filtered_entropy <= entropy_median) & (filtered_varentropy <= varentropy_median)  # Low E, Low VE\n",
    "    q2 = (filtered_entropy > entropy_median) & (filtered_varentropy <= varentropy_median)   # High E, Low VE\n",
    "    q3 = (filtered_entropy <= entropy_median) & (filtered_varentropy > varentropy_median)   # Low E, High VE\n",
    "    q4 = (filtered_entropy > entropy_median) & (filtered_varentropy > varentropy_median)    # High E, High VE\n",
    "    \n",
    "    quadrants = {\n",
    "        \"Flowing (Low E, Low VE)\": q1,\n",
    "        \"Careful (High E, Low VE)\": q2,\n",
    "        \"Exploring (Low E, High VE)\": q3,\n",
    "        \"Resampling (High E, High VE)\": q4\n",
    "    }\n",
    "    \n",
    "    # print(\"Feature Distribution in Quadrants:\")\n",
    "    for name, mask in quadrants.items():\n",
    "        n_features = mask.sum().item()\n",
    "        # print(f\"\\n{name}: {n_features} features ({n_features/len(filtered_entropy):.1%})\")\n",
    "        \n",
    "        # Get example features from this quadrant\n",
    "        feature_indices = mask.nonzero(as_tuple=True)[0]\n",
    "        if len(feature_indices) > 0:\n",
    "            sample_indices = feature_indices[torch.randperm(len(feature_indices))[:n_examples]]\n",
    "            \n",
    "            # print(\"\\nExample features:\")\n",
    "            for idx in sample_indices:\n",
    "                # Get activation statistics for this feature\n",
    "                feature_acts = acts[:, idx]\n",
    "                active_prompts = (feature_acts.abs() > config['activation_threshold']).sum().item()\n",
    "                max_activation = feature_acts.abs().max().item()\n",
    "                \n",
    "                # print(f\"\\nFeature {idx}:\")\n",
    "                # print(f\"  Entropy: {filtered_entropy[idx]:.3f}\")\n",
    "                # print(f\"  Varentropy: {filtered_varentropy[idx]:.3f}\")\n",
    "                # print(f\"  Active in {active_prompts}/{len(acts)} prompts\")\n",
    "                # print(f\"  Max activation: {max_activation:.3f}\")\n",
    "                \n",
    "    return quadrants, filtered_entropy, filtered_varentropy\n",
    "\n",
    "# Create a visualization of the quadrants\n",
    "def plot_entropy_quadrants(entropy, varentropy, quadrants):\n",
    "    \"\"\"Create a scatter plot showing feature distribution across quadrants.\"\"\"\n",
    "    import plotly.express as px\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'entropy': entropy.cpu().numpy(),\n",
    "        'varentropy': varentropy.cpu().numpy(),\n",
    "        'quadrant': 'Unknown'\n",
    "    })\n",
    "    \n",
    "    # Assign quadrant labels\n",
    "    for name, mask in quadrants.items():\n",
    "        df.loc[mask.cpu().numpy(), 'quadrant'] = name\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig = px.scatter(df, x='entropy', y='varentropy', color='quadrant',\n",
    "                    title='Feature Distribution across Entropy/Varentropy Quadrants',\n",
    "                    labels={'entropy': 'Entropy', 'varentropy': 'Variance of Entropy'})\n",
    "    \n",
    "    # Add median lines\n",
    "    fig.add_hline(y=varentropy.median().item(), line_dash=\"dash\", line_color=\"gray\")\n",
    "    fig.add_vline(x=entropy.median().item(), line_dash=\"dash\", line_color=\"gray\")\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadrants, filtered_entropy, filtered_varentropy = analyze_feature_quadrants(filtered_acts)\n",
    "plot_entropy_quadrants(filtered_entropy, filtered_varentropy, quadrants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying UMAP and HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap_preprocessing(acts, preprocessing_config):\n",
    "    \"\"\"\n",
    "    Apply UMAP dimensionality reduction if beneficial.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    feature_acts = acts.T.cpu().numpy()\n",
    "    normalized_acts = feature_acts / (np.linalg.norm(feature_acts, axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    n_samples, n_dims = normalized_acts.shape\n",
    "    min_dims_for_reduction = 50  # Only reduce if we have more than 50 dimensions\n",
    "    \n",
    "    if n_dims <= min_dims_for_reduction:\n",
    "        print(f\"Skipping UMAP reduction - input dimensionality ({n_dims}) is already manageable\")\n",
    "        return normalized_acts, normalized_acts\n",
    "        \n",
    "    # For high-dimensional data, apply UMAP reduction\n",
    "    umap_config = preprocessing_config.get('umap', {})\n",
    "    target_dims = min(umap_config.get('n_components', 50), n_samples - 1)\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_components=target_dims,\n",
    "        n_neighbors=min(umap_config.get('n_neighbors', 15), n_samples - 1),\n",
    "        min_dist=umap_config.get('min_dist', 0.1),\n",
    "        metric=umap_config.get('metric', 'cosine'),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        reduced_acts = reducer.fit_transform(normalized_acts)\n",
    "        print(f\"Applied UMAP reduction: {n_dims}d → {target_dims}d\")\n",
    "        return reduced_acts, normalized_acts\n",
    "    except Exception as e:\n",
    "        print(f\"Error during UMAP reduction: {e}\")\n",
    "        return normalized_acts, normalized_acts\n",
    "\n",
    "\n",
    "def run_hdbscan(reduced_acts, clustering_config):\n",
    "    \"\"\"Run HDBSCAN clustering.\"\"\"\n",
    "    hdbscan_config = clustering_config.get('hdbscan', {})\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=hdbscan_config.get('min_cluster_size', 5),\n",
    "        min_samples=hdbscan_config.get('min_samples', 1),\n",
    "        metric=hdbscan_config.get('metric', 'euclidean'),\n",
    "        cluster_selection_epsilon=hdbscan_config.get('cluster_selection_epsilon', 0.0)\n",
    "    )\n",
    "    return clusterer.fit_predict(reduced_acts)\n",
    "\n",
    "def cluster_features(acts, clustering_config):\n",
    "    \"\"\"Cluster features using UMAP preprocessing (if needed) and HDBSCAN.\"\"\"\n",
    "    if acts.shape[1] <= 1:\n",
    "        return np.zeros(acts.shape[1], dtype=int), acts.T.cpu().numpy()\n",
    "    \n",
    "    # Preprocess with UMAP if enabled and beneficial\n",
    "    if clustering_config.get('use_umap_preprocessing', True):\n",
    "        reduced_acts, normalized_acts = apply_umap_preprocessing(acts, clustering_config)\n",
    "    else:\n",
    "        feature_acts = acts.T.cpu().numpy()\n",
    "        normalized_acts = feature_acts / (np.linalg.norm(feature_acts, axis=1, keepdims=True) + 1e-10)\n",
    "        reduced_acts = normalized_acts\n",
    "    \n",
    "    # Run HDBSCAN clustering\n",
    "    labels = run_hdbscan(reduced_acts, clustering_config)\n",
    "    \n",
    "    n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_ratio = np.mean(labels == -1) if -1 in labels else 0\n",
    "    print(f\"Clustering complete: found {n_clusters} clusters with {noise_ratio:.2%} noise points\")\n",
    "    \n",
    "    return labels, normalized_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, reduced_acts = cluster_features(filtered_acts, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster correaltion heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize correlation matrix for filtered activations\n",
    "def plot_activation_correlations(filtered_acts, max_features=1000):\n",
    "    \"\"\"\n",
    "    Create a correlation matrix heatmap for filtered activations.\n",
    "    \n",
    "    Args:\n",
    "        filtered_acts: Tensor of shape [n_prompts, n_features]\n",
    "        max_features: Maximum number of features to include in visualization\n",
    "    \"\"\"\n",
    "    # Convert to numpy and transpose to get [n_features, n_prompts]\n",
    "    acts = filtered_acts.T.cpu().numpy()\n",
    "    \n",
    "    # If we have too many features, sample randomly\n",
    "    if acts.shape[0] > max_features:\n",
    "        indices = np.random.choice(acts.shape[0], max_features, replace=False)\n",
    "        acts = acts[indices]\n",
    "        print(f\"Sampled {max_features} features randomly for visualization\")\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = np.corrcoef(acts)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=corr_matrix,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,  # Center the colorscale at 0\n",
    "        colorbar=dict(\n",
    "            title=\"Correlation\",\n",
    "            titleside=\"right\"\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Feature Activation Correlations\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xaxis=dict(showticklabels=False),\n",
    "        yaxis=dict(showticklabels=False)\n",
    "    )\n",
    "    \n",
    "    # Add correlation statistics\n",
    "    corr_flat = corr_matrix[np.triu_indices_from(corr_matrix, k=1)]\n",
    "    mean_corr = np.mean(np.abs(corr_flat))\n",
    "    median_corr = np.median(np.abs(corr_flat))\n",
    "    high_corr = np.mean(np.abs(corr_flat) > 0.5)\n",
    "    \n",
    "    print(f\"\\nCorrelation Statistics:\")\n",
    "    print(f\"Mean absolute correlation: {mean_corr:.3f}\")\n",
    "    print(f\"Median absolute correlation: {median_corr:.3f}\")\n",
    "    print(f\"Fraction of high correlations (|r| > 0.5): {high_corr:.1%}\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# You can also look at correlations within specific clusters\n",
    "def plot_cluster_correlations(filtered_acts, labels, cluster_id, max_features=1000):\n",
    "    \"\"\"Plot correlation matrix for features within a specific cluster\"\"\"\n",
    "    cluster_mask = labels == cluster_id\n",
    "    cluster_acts = filtered_acts[:, cluster_mask]\n",
    "    \n",
    "    print(f\"\\nAnalyzing Cluster {cluster_id}\")\n",
    "    print(f\"Number of features: {cluster_acts.shape[1]}\")\n",
    "    \n",
    "    plot_activation_correlations(cluster_acts, max_features)\n",
    "\n",
    "# Example: Plot correlations for a specific cluster\n",
    "# Replace 0 with any cluster ID you're interested in\n",
    "plot_cluster_correlations(filtered_acts, labels, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster prompt analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results for each cluster\n",
    "cluster_analysis = {}\n",
    "\n",
    "# Get unique valid cluster labels (including noise points)\n",
    "all_labels = np.unique(labels)\n",
    "unique_labels = [label for label in all_labels if label != -1]\n",
    "\n",
    "# Calculate global statistics\n",
    "total_features = len(labels)\n",
    "noise_points = np.sum(labels == -1)\n",
    "noise_ratio = noise_points / total_features if total_features > 0 else 0\n",
    "\n",
    "print(f\"\\n=== Cluster Analysis Summary ===\")\n",
    "print(f\"Total Features: {total_features}\")\n",
    "print(f\"Number of Clusters: {len(unique_labels)}\")\n",
    "print(f\"Noise Points: {noise_points} ({noise_ratio:.2%})\")\n",
    "\n",
    "# For each cluster (including noise)\n",
    "for label in all_labels:\n",
    "    cluster_type = \"Noise Cluster\" if label == -1 else f\"Cluster {label}\"\n",
    "    cluster_mask = labels == label\n",
    "    cluster_size = np.sum(cluster_mask)\n",
    "    cluster_indices = original_indices[cluster_mask]\n",
    "    \n",
    "    print(f\"\\n=== {cluster_type} ===\")\n",
    "    print(f\"Size: {cluster_size} features ({(cluster_size/total_features):.2%} of total)\")\n",
    "    print(f\"Feature Indices: {cluster_indices.tolist()}\")\n",
    "    \n",
    "    if label == -1:  # Skip activation analysis for noise cluster\n",
    "        cluster_analysis[label] = {\n",
    "            'size': cluster_size,\n",
    "            'indices': cluster_indices.tolist(),\n",
    "            'ratio': cluster_size/total_features,\n",
    "            'is_noise': True\n",
    "        }\n",
    "        continue\n",
    "        \n",
    "    # Get activations for all prompts on this cluster's features\n",
    "    print(reduced_acts.shape)\n",
    "    print(cluster_mask.shape)\n",
    "    cluster_acts = filtered_acts[:, cluster_mask]  # [n_prompts, n_cluster_features]\n",
    "    \n",
    "    # Compute activation statistics\n",
    "    mean_activation = torch.mean(torch.abs(cluster_acts)).item()\n",
    "    max_activation = torch.max(torch.abs(cluster_acts)).item()\n",
    "    std_activation = torch.std(torch.abs(cluster_acts)).item()\n",
    "    sparsity = (torch.abs(cluster_acts) > 0.1).float().mean().item()\n",
    "    \n",
    "    print(f\"\\nActivation Statistics:\")\n",
    "    print(f\"  Mean Activation: {mean_activation:.4f}\")\n",
    "    print(f\"  Max Activation: {max_activation:.4f}\")\n",
    "    print(f\"  Std Deviation: {std_activation:.4f}\")\n",
    "    print(f\"  Sparsity: {sparsity:.4f}\")\n",
    "    \n",
    "    # Compute average activation of each prompt on this cluster's features\n",
    "    prompt_activations = torch.mean(torch.abs(cluster_acts), dim=1)  # [n_prompts]\n",
    "    \n",
    "    # Find top activating prompts\n",
    "    top_k = min(5, len(prompts))\n",
    "    top_prompt_indices = torch.argsort(prompt_activations, descending=True)[:top_k]\n",
    "    top_prompts = [(prompts[i], prompt_activations[i].item()) for i in top_prompt_indices]\n",
    "    \n",
    "    print(f\"\\nTop {top_k} Activating Prompts:\")\n",
    "    for i, (prompt, act) in enumerate(top_prompts, 1):\n",
    "        # Show first 100 chars, with special formatting for section headers\n",
    "        truncated = prompt[:100]\n",
    "        if len(prompt) > 100:\n",
    "            truncated += \"...\"\n",
    "        \n",
    "        # Format section headers more clearly\n",
    "        if \"=\" in truncated:\n",
    "            sections = [s.strip() for s in truncated.split(\"=\") if s.strip()]\n",
    "            if sections:\n",
    "                truncated = f\"[SECTION] {' > '.join(sections)}\"\n",
    "        \n",
    "        # Split into tokens if the prompt contains spaces\n",
    "        tokens = truncated.split()\n",
    "        if len(tokens) > 15:\n",
    "            token_display = \" \".join(tokens[:15]) + \" ...\"\n",
    "        else:\n",
    "            token_display = truncated\n",
    "            \n",
    "        print(f\"  {i}. \\\"{token_display}\\\"\")\n",
    "        print(f\"     Activation: {act:.4f}\")\n",
    "        print(f\"     Total Length: {len(prompt)} chars, {len(prompt.split())} tokens\")\n",
    "    # Store detailed results\n",
    "    cluster_analysis[label] = {\n",
    "        'size': cluster_size,\n",
    "        'indices': cluster_indices.tolist(),\n",
    "        'ratio': cluster_size/total_features,\n",
    "        'is_noise': False,\n",
    "        'mean_activation': mean_activation,\n",
    "        'max_activation': max_activation,\n",
    "        'std_activation': std_activation,\n",
    "        'sparsity': sparsity,\n",
    "        'top_prompts': top_prompts,\n",
    "        'prompt_activations': prompt_activations.cpu().numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "# Print cluster similarity analysis\n",
    "if len(unique_labels) > 1:\n",
    "    print(\"\\n=== Cluster Similarity Analysis ===\")\n",
    "    for i, label1 in enumerate(unique_labels):\n",
    "        for label2 in unique_labels[i+1:]:\n",
    "            acts1 = cluster_analysis[label1]['prompt_activations']\n",
    "            acts2 = cluster_analysis[label2]['prompt_activations']\n",
    "            correlation = np.corrcoef(acts1, acts2)[0, 1]\n",
    "            if abs(correlation) > 0.5:  # Only show significant correlations\n",
    "                print(f\"Clusters {label1} and {label2}: correlation = {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature correlations more directly\n",
    "def analyze_feature_correlations(filtered_acts, original_indices, labels, threshold=0.7, max_features=1000):\n",
    "    \"\"\"\n",
    "    Identify highly correlated feature pairs directly from the correlation matrix.\n",
    "    \n",
    "    Args:\n",
    "        filtered_acts: Tensor of shape [n_prompts, n_features]\n",
    "        original_indices: Original indices of the filtered features\n",
    "        threshold: Correlation threshold to consider (absolute value)\n",
    "        max_features: Maximum number of features to include in analysis\n",
    "    \"\"\"\n",
    "    # Convert to numpy and transpose to get [n_features, n_prompts]\n",
    "    acts = filtered_acts.T.cpu().numpy()\n",
    "    \n",
    "    # If we have too many features, sample randomly\n",
    "    if acts.shape[0] > max_features:\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        indices = np.random.choice(acts.shape[0], max_features, replace=False)\n",
    "        acts = acts[indices]\n",
    "        feature_indices = original_indices[indices]\n",
    "        print(f\"Sampled {max_features} features randomly for analysis\")\n",
    "    else:\n",
    "        feature_indices = original_indices\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = np.corrcoef(acts)\n",
    "    n_features = corr_matrix.shape[0]\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    # We'll only look at the upper triangle to avoid duplicates\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):  # Upper triangle only\n",
    "            corr = corr_matrix[i, j]\n",
    "            if abs(corr) >= threshold:\n",
    "                high_corr_pairs.append((i, j, corr))\n",
    "    \n",
    "    # Sort by absolute correlation (highest first)\n",
    "    high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(high_corr_pairs)} feature pairs with |correlation| >= {threshold}\")\n",
    "    \n",
    "    # Print top correlated pairs\n",
    "    top_n = min(20, len(high_corr_pairs))\n",
    "    if top_n > 0:\n",
    "        print(f\"\\nTop {top_n} correlated feature pairs:\")\n",
    "        for i, j, corr in high_corr_pairs[:top_n]:\n",
    "            orig_i = feature_indices[i]\n",
    "            orig_j = feature_indices[j]\n",
    "            print(f\"Features {orig_i} and {orig_j}: correlation = {corr:.4f}\")\n",
    "            \n",
    "            # If we have cluster labels, show which clusters these features belong to\n",
    "            if 'labels' in globals():\n",
    "                cluster_i = labels[orig_i] if orig_i < len(labels) else \"Unknown\"\n",
    "                cluster_j = labels[orig_j] if orig_j < len(labels) else \"Unknown\"\n",
    "                print(f\"  Cluster assignments: {cluster_i} and {cluster_j}\")\n",
    "    \n",
    "    # Analyze correlation structure\n",
    "    # Count how many features have at least one strong correlation\n",
    "    features_with_strong_corr = set()\n",
    "    for i, j, _ in high_corr_pairs:\n",
    "        features_with_strong_corr.add(i)\n",
    "        features_with_strong_corr.add(j)\n",
    "    \n",
    "    print(f\"\\nFeatures with at least one strong correlation: {len(features_with_strong_corr)} out of {n_features} ({len(features_with_strong_corr)/n_features:.1%})\")\n",
    "    \n",
    "    # Find correlation hubs (features with many strong correlations)\n",
    "    corr_counts = {}\n",
    "    for i, j, _ in high_corr_pairs:\n",
    "        corr_counts[i] = corr_counts.get(i, 0) + 1\n",
    "        corr_counts[j] = corr_counts.get(j, 0) + 1\n",
    "    \n",
    "    # Sort features by number of strong correlations\n",
    "    sorted_features = sorted(corr_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if sorted_features:\n",
    "        print(\"\\nTop correlation hubs (features with many strong correlations):\")\n",
    "        for i, count in sorted_features[:10]:\n",
    "            orig_i = feature_indices[i]\n",
    "            print(f\"Feature {orig_i}: {count} strong correlations\")\n",
    "            \n",
    "            # If we have cluster labels, show which cluster this feature belongs to\n",
    "            if 'labels' in globals():\n",
    "                cluster_i = labels[orig_i] if orig_i < len(labels) else \"Unknown\"\n",
    "                print(f\"  Cluster assignment: {cluster_i}\")\n",
    "    \n",
    "    # Identify correlation communities (groups of features that are all strongly correlated with each other)\n",
    "    # This is a simple approach - for larger datasets you might want to use a community detection algorithm\n",
    "    def find_communities(pairs, n_features):\n",
    "        # Create an adjacency matrix\n",
    "        adj_matrix = np.zeros((n_features, n_features), dtype=bool)\n",
    "        for i, j, _ in pairs:\n",
    "            adj_matrix[i, j] = True\n",
    "            adj_matrix[j, i] = True\n",
    "        \n",
    "        # Find connected components (simple BFS)\n",
    "        visited = np.zeros(n_features, dtype=bool)\n",
    "        communities = []\n",
    "        \n",
    "        for start in range(n_features):\n",
    "            if visited[start]:\n",
    "                continue\n",
    "                \n",
    "            # BFS to find connected component\n",
    "            community = []\n",
    "            queue = [start]\n",
    "            visited[start] = True\n",
    "            \n",
    "            while queue:\n",
    "                node = queue.pop(0)\n",
    "                community.append(node)\n",
    "                \n",
    "                for neighbor in np.where(adj_matrix[node])[0]:\n",
    "                    if not visited[neighbor]:\n",
    "                        visited[neighbor] = True\n",
    "                        queue.append(neighbor)\n",
    "            \n",
    "            if len(community) > 1:  # Only consider communities with at least 2 features\n",
    "                communities.append(community)\n",
    "        \n",
    "        return communities\n",
    "    \n",
    "    communities = find_communities(high_corr_pairs, n_features)\n",
    "    communities.sort(key=len, reverse=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(communities)} correlation communities (groups of strongly correlated features)\")\n",
    "    \n",
    "    for i, community in enumerate(communities[:5]):  # Show top 5 communities\n",
    "        if len(community) > 1:\n",
    "            print(f\"\\nCommunity {i+1}: {len(community)} features\")\n",
    "            orig_indices = [feature_indices[j] for j in community]\n",
    "            print(f\"  Original indices: {orig_indices}\")\n",
    "            \n",
    "\n",
    "            cluster_counts = {}\n",
    "            for idx in orig_indices:\n",
    "                if idx < len(labels):\n",
    "                    cluster = labels[idx]\n",
    "                    cluster_counts[cluster] = cluster_counts.get(cluster, 0) + 1\n",
    "            \n",
    "            print(\"  Cluster distribution:\")\n",
    "            for cluster, count in sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    Cluster {cluster}: {count} features ({count/len(community):.1%})\")\n",
    "    \n",
    "    return high_corr_pairs, feature_indices, communities\n",
    "\n",
    "# Run the analysis with a threshold of 0.7\n",
    "high_corr_pairs, feature_indices, communities = analyze_feature_correlations(filtered_acts, original_indices, labels, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one of the correlation communities\n",
    "def visualize_correlation_community(filtered_acts, community_indices, feature_indices, prompts, n_top_prompts=5):\n",
    "    \"\"\"Visualize a community of correlated features\"\"\"\n",
    "    # Get the original indices for the community\n",
    "    orig_indices = [feature_indices[i] for i in community_indices]\n",
    "    \n",
    "    # Get activations for these features - use community_indices directly since these are indices into filtered_acts\n",
    "    community_acts = np.array([filtered_acts[:, i].cpu().numpy() for i in community_indices]).T\n",
    "    \n",
    "    # Create a heatmap of feature activations across prompts\n",
    "    # First, find top activating prompts for any feature in the community\n",
    "    max_activations = np.max(np.abs(community_acts), axis=1)\n",
    "    top_prompt_indices = np.argsort(max_activations)[::-1][:n_top_prompts]\n",
    "    \n",
    "    # Create a heatmap of these top prompts vs features\n",
    "    top_acts = community_acts[top_prompt_indices]\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=top_acts,\n",
    "        x=[f\"Feature {idx}\" for idx in orig_indices],\n",
    "        y=[f\"Prompt {i+1}\" for i in range(len(top_prompt_indices))],\n",
    "        colorscale='RdBu',\n",
    "        zmid=0\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Activation Patterns for Correlation Community (Size: {len(community_indices)})\",\n",
    "        width=max(800, 100 + 50 * len(community_indices)),\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print the top prompts\n",
    "    print(\"\\nTop activating prompts for this community:\")\n",
    "    for i, prompt_idx in enumerate(top_prompt_indices):\n",
    "        prompt = prompts[prompt_idx]\n",
    "        max_act = max_activations[prompt_idx]\n",
    "        \n",
    "        # Truncate prompt for display\n",
    "        if len(prompt) > 100:\n",
    "            display_prompt = prompt[:100] + \"...\"\n",
    "        else:\n",
    "            display_prompt = prompt\n",
    "            \n",
    "        print(f\"{i+1}. \\\"{display_prompt}\\\"\")\n",
    "        print(f\"   Max activation: {max_act:.4f}\")\n",
    "\n",
    "# Visualize the largest correlation community\n",
    "if len(high_corr_pairs) > 0 and 'communities' in locals() and communities:\n",
    "    largest_community = communities[1]\n",
    "    visualize_correlation_community(filtered_acts, largest_community, feature_indices, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
