{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview & Current Questions\n",
    "This is for validating my clustering idea:\n",
    "1. Store SAE activations for all prompts\n",
    "2. Preprocess activations based on entropy and activation level (optional?)\n",
    "3. Cluster activations using UMAP and HDBSCAN\n",
    "4. Analyze clusters\n",
    "\n",
    "Some questions: \n",
    "1. How does entropy, activation preprocessing affect clustering? How does n_prompts, length, affect this step?\n",
    "2. How useful is UMAP? What are the best parameters?\n",
    "3. How does HDBSCAN perform? What are the best parameters?\n",
    "4. How does the clustering change when we use different datasets?\n",
    "5. What commonalities do clusters have? Does this vary by dataset? Hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, config, model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import torch\n",
    "import gc\n",
    "from config import config # cfg auto updates\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap.umap_ as umap\n",
    "import plotly.graph_objects as go\n",
    "from einops import rearrange\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HookedSAETransformer.from_pretrained(\"EleutherAI/pythia-70m-deduped\", device=device)\n",
    "sae, _, _ = SAE.from_pretrained(\n",
    "    release=\"pythia-70m-deduped-mlp-sm\",\n",
    "    sae_id=\"blocks.3.hook_mlp_out\",\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_invalidate_cache(cached_data, n_prompts):\n",
    "    \"\"\"Check if cache should be invalidated based on n_prompts.\"\"\"\n",
    "    if cached_data is None:\n",
    "        return True\n",
    "    return cached_data['acts'].shape[0] < n_prompts * 0.85\n",
    "\n",
    "def load_diverse_prompts(config):\n",
    "    \"\"\"Load diverse prompts from multiple sources for maximum SAE latent activation.\n",
    "    \n",
    "    Sources include:\n",
    "    - Wikipedia (academic/factual)\n",
    "    - C4 (web text)\n",
    "    - BookCorpus (fiction/narrative)\n",
    "    - Wikitext (structured wiki text)\n",
    "    - HuggingFace Datasets:\n",
    "        - code (programming)\n",
    "        - scientific papers\n",
    "        - news articles\n",
    "        - social media\n",
    "        - dialogue\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "    import random\n",
    "    import re\n",
    "    \n",
    "    n_prompts = config.get('n_prompts', 500)\n",
    "    prompts = []\n",
    "    \n",
    "    datasets = [\n",
    "        # General knowledge / factual\n",
    "        ('wikipedia', '20220301.en', 'train'),  # Wikipedia articles\n",
    "        ('wikitext', 'wikitext-103-raw-v1', 'train'),  # More wiki text\n",
    "        \n",
    "        # Web/social text\n",
    "        ('allenai/c4', 'en', 'train'),  # Web text\n",
    "        ('reddit', None, 'train'),  # Social media discussions\n",
    "        ('tweet_eval', 'sentiment', 'train'),  # Twitter posts\n",
    "        \n",
    "        # Books and stories\n",
    "        ('bookcorpus', None, 'train'),  # Fiction books\n",
    "        \n",
    "        # Technical/specialized\n",
    "        ('code_search_net', 'python', 'train'),  # Code + documentation\n",
    "        ('glue', 'cola', 'train'),  # Linguistic examples - need to access 'sentence' field\n",
    "        \n",
    "        # Dialogue and conversation\n",
    "        ('daily_dialog', None, 'train'),  # Conversations\n",
    "        ('squad', 'plain_text', 'train'),  # Question-answer pairs - need plain_text config\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    prompts_per_dataset = n_prompts // len(datasets)\n",
    "    seen_content = set()  # Track unique content\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def is_diverse_enough(text, seen_content):\n",
    "        \"\"\"Check if text is diverse enough from existing content.\"\"\"\n",
    "        # Skip very short texts\n",
    "        if len(text) < config.get('min_prompt_length', 50):\n",
    "            return False\n",
    "            \n",
    "        # Skip very long texts\n",
    "        if len(text) > config.get('max_prompt_length', 1000):\n",
    "            return False\n",
    "            \n",
    "        # Check for near-duplicates using character n-grams\n",
    "        ngram_size = 10\n",
    "        text_ngrams = set(text[i:i+ngram_size] for i in range(len(text)-ngram_size+1))\n",
    "        \n",
    "        # Calculate overlap with existing content\n",
    "        for existing in seen_content:\n",
    "            existing_ngrams = set(existing[i:i+ngram_size] for i in range(len(existing)-ngram_size+1))\n",
    "            overlap = len(text_ngrams & existing_ngrams) / len(text_ngrams)\n",
    "            if overlap > 0.5:  # More than 50% overlap\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    for name, subset, split in datasets:\n",
    "        try:\n",
    "            print(f\"Loading prompts from {name}...\")\n",
    "            if subset:\n",
    "                dataset = load_dataset(name, subset, split=split, streaming=True)\n",
    "            else:\n",
    "                dataset = load_dataset(name, split=split, streaming=True)\n",
    "            \n",
    "            dataset_prompts = []\n",
    "            # Determine the correct text field based on dataset\n",
    "            if name == 'code_search_net':\n",
    "                text_key = 'whole_func_string'\n",
    "            elif name == 'glue':\n",
    "                text_key = 'sentence'  # GLUE uses 'sentence' field\n",
    "            elif name == 'squad':\n",
    "                text_key = 'context'  # SQuAD uses 'context' field\n",
    "            elif name == 'tweet_eval':\n",
    "                text_key = 'text'\n",
    "            elif name == 'daily_dialog':\n",
    "                text_key = 'dialog'\n",
    "            else:\n",
    "                text_key = 'text' if 'text' in next(iter(dataset)) else 'content'\n",
    "            \n",
    "            for item in dataset:\n",
    "                text = item[text_key]\n",
    "                if isinstance(text, list):  # Handle list-type fields (e.g., dialogues)\n",
    "                    text = \" \".join(text)\n",
    "                \n",
    "                text = clean_text(text)\n",
    "                if text and is_diverse_enough(text, seen_content):\n",
    "                    dataset_prompts.append(text)\n",
    "                    seen_content.add(text)\n",
    "                \n",
    "                if len(dataset_prompts) >= prompts_per_dataset:\n",
    "                    break\n",
    "            \n",
    "            prompts.extend(dataset_prompts)\n",
    "            print(f\"Added {len(dataset_prompts)} prompts from {name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Final deduplication and shuffling\n",
    "    prompts = list(set(prompts))\n",
    "    random.shuffle(prompts)\n",
    "    prompts = prompts[:n_prompts]\n",
    "    \n",
    "    # Print diversity statistics\n",
    "    print(f\"\\nTotal unique prompts loaded: {len(prompts)}\")\n",
    "    print(\"\\nPrompt statistics:\")\n",
    "    lengths = [len(p.split()) for p in prompts]\n",
    "    print(f\"Average words per prompt: {sum(lengths)/len(lengths):.1f}\")\n",
    "    print(f\"Min length: {min(lengths)}, Max length: {max(lengths)}\")\n",
    "    \n",
    "    # Sample and print some prompts for inspection\n",
    "    print(\"\\nSample prompts:\")\n",
    "    for p in random.sample(prompts, min(5, len(prompts))):\n",
    "        print(f\"- {p[:100]}...\")\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def get_cache_filename(config, n_prompts):\n",
    "    \"\"\"Generate a cache filename based on hierarchical config parameters.\"\"\"\n",
    "    # Create a unique filename based on key parameters\n",
    "    \n",
    "    params = [\n",
    "        f\"prompts_{n_prompts}\",\n",
    "    ]\n",
    "    print(params)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    cache_dir = config.get('cache_dir', 'cache')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    return os.path.join(cache_dir, f\"processed_data_{'_'.join(params)}.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    \"\"\"Clear CUDA cache to free memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def create_embed_hook(P):\n",
    "    def hook(value, hook):\n",
    "        return P.unsqueeze(0)\n",
    "    return hook\n",
    "\n",
    "def get_feature_activations(model, sae, tokens, P=None):\n",
    "    hooks = [('hook_embed', create_embed_hook(P))] if P is not None else []\n",
    "    with model.hooks(fwd_hooks=hooks):\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens, \n",
    "            saes=[sae],\n",
    "            names_filter=lambda name: name == 'blocks.3.hook_mlp_out.hook_sae_acts_post'  # Only cache what you need\n",
    "        )\n",
    "    return cache['blocks.3.hook_mlp_out.hook_sae_acts_post']\n",
    "\n",
    "def get_model_activations(model, tokens):\n",
    "    _, cache = model.run_with_cache(\n",
    "        tokens,\n",
    "        names_filter=lambda name: name == 'blocks.3.hook_mlp_out'  # Only cache what you need\n",
    "    )\n",
    "    return cache['blocks.3.hook_mlp_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activations(model, sae, prompts, config):\n",
    "    \"\"\"Collect feature activations from prompts.\"\"\"\n",
    "    all_acts = []\n",
    "    batch_size = config.get('batch_size', 10)\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{len(prompts) // batch_size + 1}...\")\n",
    "        batch_acts = []\n",
    "        for prompt in batch_prompts:\n",
    "            try:\n",
    "                tokens = model.to_tokens(prompt)\n",
    "                acts = get_feature_activations(model, sae, tokens)\n",
    "                #acts = get_model_activations(model, tokens)\n",
    "                batch_acts.append(acts.mean(dim=1).squeeze(0)) # dont care about position specific information\n",
    "                clear_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping prompt '{e}\")\n",
    "        \n",
    "        if batch_acts:\n",
    "            all_acts.extend(batch_acts)\n",
    "            \n",
    "    acts = torch.stack(all_acts)\n",
    "    print(f\"Collected activations for {acts.shape[0]} prompts, {acts.shape[1]} features\")\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompts_25000']\n",
      "No cached data found at feature_cache/processed_data_prompts_25000.pt\n",
      "True\n",
      "Processing data from scratch with 25000 prompts\n",
      "Loading prompts from wikipedia...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data from scratch with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_prompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m \u001b[43mload_diverse_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     acts \u001b[38;5;241m=\u001b[39m collect_activations(model, sae, prompts, config)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Save the new data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 113\u001b[0m, in \u001b[0;36mload_diverse_prompts\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     text_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m    114\u001b[0m     text \u001b[38;5;241m=\u001b[39m item[text_key]\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# Handle list-type fields (e.g., dialogues)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/site-packages/datasets/iterable_dataset.py:2033\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[0;32m-> 2033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m:\n\u001b[1;32m   2034\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   2037\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   2038\u001b[0m         )\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m format_dict(example) \u001b[38;5;28;01mif\u001b[39;00m format_dict \u001b[38;5;28;01melse\u001b[39;00m example\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/site-packages/datasets/arrow_dataset.py:208\u001b[0m, in \u001b[0;36mDatasetInfoMixin.features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeatures\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Features]:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/site-packages/datasets/features/features.py:2110\u001b[0m, in \u001b[0;36mFeatures.copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;124;03m    Make a deep copy of [`Features`].\u001b[39;00m\n\u001b[1;32m   2095\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/slens/lib/python3.10/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;66;03m# aha, this is the first one :-)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m         memo[\u001b[38;5;28mid\u001b[39m(memo)]\u001b[38;5;241m=\u001b[39m[x]\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reconstruct\u001b[39m(x, memo, func, args,\n\u001b[1;32m    260\u001b[0m                  state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, listiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dictiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m                  \u001b[38;5;241m*\u001b[39m, deepcopy\u001b[38;5;241m=\u001b[39mdeepcopy):\n\u001b[1;32m    262\u001b[0m     deep \u001b[38;5;241m=\u001b[39m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Determine whether to use cached data \n",
    "n_prompts = config['n_prompts']\n",
    "use_cached_data = config['use_cached_data']\n",
    "cache_filename = get_cache_filename(config, n_prompts)\n",
    "cached_data = load_processed_data(cache_filename)\n",
    "\n",
    "print(should_invalidate_cache(cached_data, n_prompts))\n",
    "\n",
    "\n",
    "if use_cached_data and cached_data is not None: #and not should_invalidate_cache(cached_data, n_prompts):\n",
    "    print(f\"Using cached data with {cached_data['acts'].shape[0]} prompts\")\n",
    "    acts = cached_data['acts']\n",
    "    prompts = cached_data.get('prompts', [])\n",
    "else:\n",
    "    print(f\"Processing data from scratch with {n_prompts} prompts\")\n",
    "    prompts = load_diverse_prompts(config)\n",
    "    acts = collect_activations(model, sae, prompts, config)\n",
    "    \n",
    "    # Save the new data\n",
    "    torch.save({\n",
    "        'acts': acts,\n",
    "        'prompts': prompts\n",
    "    }, cache_filename)\n",
    "    print(f\"Data saved successfully to {cache_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
